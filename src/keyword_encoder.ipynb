{"nbformat":4,"nbformat_minor":0,"metadata":{"@webio":{"lastCommId":null,"lastKernelId":null},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"keyword_encoder.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"1s14dawJhKSl","colab_type":"code","outputId":"79f6f81d-ef92-40f6-c57d-dd74aa2391eb","executionInfo":{"status":"ok","timestamp":1588694039056,"user_tz":240,"elapsed":823,"user":{"displayName":"Sayan Samanta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOuT4BQxbOWcYTW_r0RQ02DqKvgrpoyxBKqxPAUi8=s64","userId":"03850430243742702686"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e2739BaShkWC","colab_type":"code","colab":{}},"source":["!pip install -q ray"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkdNka0VhHMD","colab_type":"code","colab":{}},"source":["import spacy\n","import csv\n","import re\n","import ray\n","import multiprocessing\n","from functools import partial\n","from tqdm.notebook import tqdm\n","from itertools import chain\n","from random import random, shuffle, randint\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qIAA1LQ5hHMT","colab_type":"code","colab":{}},"source":["DELIMS = {\n","    'section': '~',\n","    'category': '`',\n","    'keywords': '^',\n","    'title': '@',\n","    'body': '}'\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q70qfDvhhHMh","colab_type":"code","colab":{}},"source":["PRONOUN_LIST = ['I', 'Me', 'We', 'You', 'He', 'She',\n","                'It', 'Him', 'Her', 'Them', 'They']\n","PRONOUNS = set(PRONOUN_LIST + [x.lower() for x in PRONOUN_LIST])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WE77U9HhHMq","colab_type":"code","colab":{}},"source":["import dask.dataframe as ddf\n","import dask.multiprocessing\n","from dask.distributed import Client\n","\n","def encode_keywords(csv_path, model='en_core_web_sm',\n","                    category_field=None,\n","                    keywords_field=None,\n","                    title_field=None,\n","                    body_field=None,\n","                    keyword_gen='title',\n","                    keyword_sep=',',\n","                    dropout=0.5,\n","                    repeat=3,\n","                    max_keywords=3,\n","                    keyword_length_max=20,\n","                    out_path='csv_encoded.txt',\n","                    start_token=\"<|startoftext|>\",\n","                    end_token=\"<|endoftext|>\"):\n","    df = []\n","    print('Loading Text...')\n","#     client = Client(n_workers = multiprocessing.cpu_count(), )\n","#     df = ddf.read_csv('../data/Clothing_Shoes_and_Jewelry_5.tsv',  # read in parallel\n","#              sep='\\t', \n","#              blocksize=1000000,\n","#              )\n","#     df = df.sample(frac=0.1, replace=False, random_state=42)\n","#     df = df.compute(scheduler=client)\n","#     df = df.values.tolist()\n","    \n","    with open(csv_path, 'r', encoding='utf8', errors='ignore') as f:\n","        reader = csv.DictReader(f,delimiter='\\t')\n","        for row in tqdm(reader):\n","            df.append(row)\n","    \n","    def chunker(seq, size):\n","        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n","    \n","    num_threads = multiprocessing.cpu_count() * 2  # colocate 2 processes per thread\n","    print(\"Starting up {} Workers\".format(num_threads))\n","\n","    encoders = [Encoder.remote(model, category_field,\n","                               keywords_field,\n","                               title_field,\n","                               body_field,\n","                               keyword_gen,\n","                               keyword_sep,\n","                               repeat,\n","                               max_keywords,\n","                               keyword_length_max,\n","                               start_token,\n","                               end_token,\n","                               DELIMS,\n","                               PRONOUNS) for _ in range(num_threads)]\n","    \n","    \n","    with open(out_path, 'w', encoding='utf8', errors='ignore') as w:\n","        pbar = tqdm(total=len(df), smoothing=0)\n","        for chunk in chunker(df, num_threads):\n","            results = ray.get([c.generate_encoded_text.remote(row)\n","                               for c, row in list(zip(encoders, chunk))])\n","\n","            # unnest and randomize results\n","            results = list(chain.from_iterable(results))\n","            shuffle(results)\n","            for result in results:\n","                w.write(result)\n","\n","            pbar.update(num_threads)\n","        pbar.close()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9RsLKbRjhHMz","colab_type":"code","colab":{}},"source":["#@ray.remote(num_cpus=0.5, )\n","class Encoder(object):\n","    def __init__(self, model, category_field,\n","                 keywords_field,\n","                 title_field,\n","                 body_field,\n","                 keyword_gen,\n","                 keyword_sep,\n","                 repeat,\n","                 max_keywords,\n","                 keyword_length_max,\n","                 start_token,\n","                 end_token,\n","                 DELIMS,\n","                 PRONOUNS):\n","        \n","        self.nlp = spacy.load(model)\n","        self.pattern = re.compile('\\W+')\n","\n","        self.category_field = category_field\n","        self.keywords_field = keywords_field\n","        self.title_field = title_field\n","        self.body_field = body_field\n","        self.keyword_gen = keyword_gen\n","        self.keyword_sep = keyword_sep\n","        self.repeat = repeat\n","        self.max_keywords = max_keywords\n","        self.keyword_length_max = keyword_length_max\n","        self.start_token = start_token\n","        self.end_token = end_token\n","        self.DELIMS = DELIMS\n","        self.PRONOUNS = PRONOUNS\n","        \n","    def build_section(self, section, text):\n","        if text is None:\n","            return ''\n","        return self.DELIMS['section'] + self.DELIMS[section] + text\n","    \n","    def generate_encoded_text(self, row):\n","\n","        nlp = self.nlp\n","        pattern = self.pattern\n","        # category should be normalized to account for user input\n","        try:\n","            category = re.sub(\n","            pattern, '-', row[self.category_field].lower().strip()) if self.category_field is not None else None\n","        except AttributeError:\n","            print(row)\n","\n","        title = row[self.title_field] if self.title_field is not None else None\n","        body = row[self.body_field] if self.body_field is not None else None\n","\n","        if self.keywords_field is None:\n","            # Generate the keywords using spacy\n","            # replace smart quotes first for better tokenization\n","            text = re.sub(u'[\\u2018\\u2019]', \"'\",\n","                          (re.sub(u'[\\u201c\\u201d]', '\"', row[self.keyword_gen])))\n","            doc = nlp(text)\n","            keywords_pos = [chunk.text if chunk.pos_ in ['NOUN', 'PROPN']\n","                            else \n","                            chunk.lemma_ if chunk.pos_ in ['VERB', 'ADJ', 'ADV']\n","                            else 'I'\n","                            for chunk in doc\n","                            if not chunk.is_stop\n","                            ]\n","            keywords_ents = [re.sub(' ', '-', chunk.text)\n","                             for chunk in doc.ents]\n","            keywords_compounds = [re.sub(' ', '-', chunk.text)\n","                                  for chunk in doc.noun_chunks\n","                                  if len(chunk.text) < self.keyword_length_max]\n","\n","            keywords = list(set(keywords_pos +\n","                                keywords_ents +\n","                                keywords_compounds) - self.PRONOUNS)  # dedupe\n","        else:\n","            keywords = [keyword.strip()\n","                        for keyword in row[self.keywords_field].split(self.keyword_sep)]\n","            keywords = list(set(keywords))\n","\n","        encoded_texts = []\n","        for _ in range(self.repeat):\n","            new_keywords = keywords\n","            shuffle(new_keywords)\n","            new_keywords = \" \".join(\n","                new_keywords[:randint(0, self.max_keywords)])\n","\n","            encoded_texts.append(self.start_token +\n","                                 self.build_section('category', category) +\n","                                 self.build_section('keywords', new_keywords) +\n","                                 self.build_section('title', title) +\n","                                 self.build_section('body', body) +\n","                                 self.end_token + \"\\n\")\n","        return encoded_texts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydi7JicChHM_","colab_type":"code","colab":{}},"source":["#ray.shutdown()\n","#ray.init(object_store_memory=100 * 1000000,\n","#         redis_max_memory=100 * 1000000)\n","#ray.init()\n","\n","encode_keywords(csv_path='/content/drive/My Drive/text-review-generation-data2040/data/Clothing_Shoes_and_Jewelry_5.tsv',\n","                out_path='/content/drive/My Drive/text-review-generation-data2040/data/Clothing_Shoes_and_Jewelry_5_encoded.txt',\n","                category_field='rating',\n","                title_field='text',\n","                keyword_gen='text') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIP9u94ghHNJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}